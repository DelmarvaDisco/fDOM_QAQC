---
title: "fDOM data correction examples"
author: "James Maze"
date: "12/14/2020"
output: html_document
Example data sets used: ND-SW from 9/6 to 10/26
---

## 1. Load packages and working directory

I didn't show this chunk for brevity's sake, but I used the following packages:

tidyverse, lubridate, zoo, xts, dygraphs, htmlwidgets, rmarkdown
```{r include=FALSE}

data_dir <- "C:/Users/James Maze/Desktop/Delmarva/Github/fDOM_QAQC_IDK/Data_Raw"

library("tidyverse")
library("lubridate")
library("zoo")
library("xts")
library("dygraphs")
library("htmlwidgets")
library("rmarkdown")

```

## 2 Organize the EXO Files

#### 2.1  Read in the EXO files

The KorEXO formatting makes the files really difficult to read in. I need to streamline this at some point, but for now, I'm removing the header in excel. That's why these file names are "no_head.csv".

Also, I read the files in separately for this vs one function running through a folder, because I feel like it gives me more flexibility while toying with different QAQC options. 

When I read the data in, I do three things:
  
  1. Rename the columns so they are easier for me to work with 
  
  2. Add a column for Flagging, which I automatically designate as 0 (i.e. no Flag). Through out the QAQC, data will get flagged. 
  
  3. Select the columns of interest. I drop three variables that we don't measure. 
  
I combine all the files using rbind. 


```{r include=FALSE}

#ND from Sep 6th through Sep 19th
ND_SW_EXO_20200919_raw <- read_csv(paste0(data_dir,"/ND_SW_fDOM_20200919_no_head.csv")) %>% 
  rename("Date" = `Date (MM/DD/YYYY)`, 
         "Time" = `Time (HH:mm:ss)`, 
         "fDOM_QSU" = `fDOM QSU`, 
         "Turb_FNU" = `Turbidity FNU`,
         "Batt_V" = `Battery V`) %>% 
  add_column(Flag = 0) %>% 
  select(c(Date, Time, fDOM_QSU, Turb_FNU, Batt_V, Flag, `Site Name`))

#ND from Sep 19th to October 9th 
ND_SW_EXO_20201009_raw <- read_csv(paste0(data_dir,"/ND_SW_fDOM_20201009_no_head.csv")) %>% 
  rename("Date" = `Date (MM/DD/YYYY)`, 
         "Time" = `Time (HH:mm:ss)`, 
         "fDOM_QSU" = `fDOM QSU`, 
         "Turb_FNU" = `Turbidity FNU`,
         "Batt_V" = `Battery V`) %>% 
  add_column(Flag = 0) %>% 
  select(c(Date, Time, fDOM_QSU, Turb_FNU, Batt_V, Flag, `Site Name`))

#ND from October 9th to October 26th
ND_SW_EXO_20201026_raw <- read_csv(paste0(data_dir,"/ND_SW_fDOM_20201026_no_head.csv")) %>% 
  rename("Date" = `Date (MM/DD/YYYY)`, 
         "Time" = `Time (HH:mm:ss)`, 
         "fDOM_QSU" = `fDOM QSU`, 
         "Turb_FNU" = `Turbidity FNU`,
         "Batt_V" = `Battery V`) %>% 
  add_column(Flag = 0) %>% 
  select(c(Date, Time, fDOM_QSU, Turb_FNU, Batt_V, Flag, `Site Name`))


# Combine all of the files
EXO_raw <- rbind(ND_SW_EXO_20201009_raw, ND_SW_EXO_20200919_raw, ND_SW_EXO_20201026_raw)

#Remove the individual files from the environment after data is binded
rm(ND_SW_EXO_20201009_raw, ND_SW_EXO_20200919_raw, ND_SW_EXO_20201026_raw)

```

#### 2.2 Convert the timestamp on the EXO files

In this chunk, I paste the Date and Time columns into a single column. Then I change the format to a POSIXct. Lastly, I filter out the negative QSU values.

```{r}
EXO_raw <- EXO_raw %>% 
  mutate(Date_Time_EST = paste(Date, Time)) %>%
  mutate(Date_Time_EST = mdy_hms(Date_Time_EST, 
                                 tz = "US/Eastern")) %>% 
  #Filtering out the wackiest values before hand
  filter(fDOM_QSU > 0) 


```
## 3. Temperature Correction 

#### 3.1 Inspect YSI's Temperature Correction 

The table relates temperature to florescence for a 300 ppb QSU standard (Source  YSI Manual). According to YSI, 22 degrees C is considered the reference temperature. As you can see, there is a clear linear relationship (r^2 = 0.9999).

For the 300 ppb standard, the formula from linear regression is:

**In y = mx + b: QSU = -1.364161(Deg C - 22) + 330.044056**

**In point-slope: QSU - 300 = -1.364161(Deg C - 22)**


```{r}

#Read the table
(Temp_QSU_slope <- read_csv(paste0(data_dir,"/YSI_Temp_QSU_corr.csv"))) 

#Plot the the points
(Temp_QSU_plot <- ggplot(data = Temp_QSU_slope, 
        mapping = aes(x = Deg_C, 
                      y = fDOM_QSU)) +
    geom_point(size = 5) +
    theme_bw() +
    labs(title = "QSU and Temperature (Standard = 300 ppb @ 22C)"))

#Do a quick linear regression (r^2 = 0.9999)
Temp_QSU_linear_reg <- lm(formula = fDOM_QSU ~ Deg_C, data = Temp_QSU_slope)
summary(Temp_QSU_linear_reg)

```

#### 3.2 Read in the co-located temperature sensors (From PME miniDOTs)

Not very smooth, but it gets the job done. Since the PME file has a row of units below the column name, its easier to slice and rename. Like the EXO files, I need to write a proper function for reading in data. For the purpose of fDOM corrections, I only keep the time (Date time) and temperature (double) column. 

Once again, I read the files in individually, because I feel like it grants more flexibility, while I test different things out. 

```{r include=FALSE}
#Read the PME file and select the columns

#ND SW from Sep 6 to Sep 19
ND_SW_PME_20200919_trimmed <- read_delim(paste0(data_dir,"/ND_SW_PME_20200906_20200919.txt"),
                             delim = ",",
                             skip = 9, 
                             col_names = FALSE) %>% 
  select(c(X3, X5)) %>% 
  rename(Date_Time_EST = X3, Temp_C = X5) %>% 
  add_column("Site Name" = "ND") 


#ND SW from Sep 19 to Oct 26
ND_SW_PME_20201026_trimmed <- read_delim(paste0(data_dir,"/ND_SW_PME_20200919_20201026.txt"),
                             delim = ",",
                             skip = 9, 
                             col_names = FALSE) %>% 
  select(c(X3, X5)) %>% 
  rename(Date_Time_EST = X3, Temp_C = X5) %>% 
  add_column("Site Name" = "ND") 
  


#Merge the PME files
PME_ready <- rbind(ND_SW_PME_20200919_trimmed, ND_SW_PME_20201026_trimmed) %>% 
#Converts the Temp_C column to numeric and the Date to POSIXCT.
  mutate("Temp_C" = as.numeric(Temp_C),
         "Date_Time_EST" = ymd_hms(Date_Time_EST, 
                                   tz = "US/Eastern"))

head(PME_ready)

rm(ND_SW_PME_20200919_trimmed, ND_SW_PME_20201026_trimmed)

```

#### 3.3 Join the co-located temp sensors to the fDOM data

Unfortunately, the MiniDOTs started on a 1 minute delay. So I had to bump the timestamps back 1 minute (e.g. Temp at 12:45 was actually measured at 12:46). For the purpose of temperature correction, this should be okay. I used an inner join, so times that didn't have an fDOM **AND** a temperature measurement were dropped.

```{r}
# PME start times were wonky, rounded to the the nearest 15 min interval. 
PME_ready <- PME_ready %>% 
  mutate("Date_Time_EST" = round_date(Date_Time_EST, "15 minute"))

#Used an inner join by timestamp and Site Name
EXO_PME_raw <- inner_join(PME_ready, 
                          EXO_raw, 
                          by = c("Date_Time_EST", "Site Name"))

#Plot temperature at the different sites to make sure the join worked properly. 
head(EXO_PME_raw)
rm(PME_ready, EXO_raw)

```

#### 3.4 Apply the linear temp correction

Definitely be vigilant for algebra mistakes that I may have made. 

1. To start, I used the point-slope equation from 3.1, where m = -1.364161.

**Equation 1) QSU Signal - 300 = m(T-22)**

2. Since the relationship from the YSI manual is for a 300 ppb standard, I need to put it in terms of percent error. Otherwise, we can't apply it to real data. 

% Error = (QSU Signal - QSU Actual)/QSU Actual * 100

Since there was a 300 ppb standard, QSU Actual = 300, which we can plug in.

**Equation 2) % Error = (QSU Signal - 300)/300 * 100**


3. Use substitution for Equations 1 and 2...

**% Error = (QSU Signal - 300)/300 * 100 = m(T - 22) * 100/300 **

4. Simplify to get...

**% Error = m/3(T -22)**

5. Once we have the percent error attributed to Temperature, we correct the fDOM data with...

**QSU_corrected = fDOM_QSU + (% Error/100 * fDOM_QSU)**


The following chunk of code runs the math generating two new columns for the Temp % Error and the temperature corrected fDOM
```{r include=FALSE}
# Prep the dataframe for temp function 
df <- EXO_PME_raw %>% 
  #Create cols of interest
  mutate("Timestamp" = ymd_hms(Date_Time_EST),
         "value" = fDOM_QSU,
         "corr" = "raw") %>% 
  #Select Cols of interest
  select(Timestamp, value, corr, Temp_C)

m <- -1.364161

temp <- fun_temp(df)

df <- df %>% 
  select(-Temp_C)  
  
df <- rbind(temp, df) %>% 
  mutate("corr" = as.character(corr))

(ggplot(data = df,
       mapping = aes(x = Timestamp, 
                     y = value,
                     color = corr)) +
  geom_line() +
  theme_bw())

```


## 4. Using the Anomaly_funtion to remove weird values. 

Could be another option for dampening noise. 
```{r}

#Prep the data for the anomaly function
anomaly <- df %>% 
  select(value, Timestamp, corr) %>% 
  filter(corr == "temp")
  

#Set thresholds for values to deviate from the rolling median
min <- -5
max <- 5

#run the function
anomaly <- fun_anomalous(anomaly, min, max)

df <- bind_rows(anomaly, df) 


ggplot(data = df,
       mapping = aes(x = Timestamp,
                     y = value,
                     color = corr)) +
  geom_line() +
  theme_bw()

```



## 5. Fouling Correction

For DK-SW, there was not evidence of fouling. The signal was 129.52 QSU at the end of the first deployment (Sep 6th - Sep 19th) and 129.56 QSU at the beginning of the second deployment (Sep 19th - Oct 29th). 

ND-SW was a different story... It appears the fouling dampens the QSU signal, because the value is higher post-clean. The USGS manual has a good approach for this. (https://pubs.usgs.gov/tm/2006/tm1D3/pdf/TM1D3.pdf) For more information, see Figure 8 on pg 26 and equation 5 on pg 32. 

The major assumption for the fouling correction is that the change in fouling is approximately linear (i.e. **Fouling(t) = t X Fouling Rate**)

#### 4.1 Calculate the total fouling that occured in each period between cleanings. This is found by:

Total fouling = QSU Post Clean - QSU Pre Clean. 

Fouling Rate = (Total Fouling)/Elapsed Time

Fouling Correction(t) = Fouling Rate * t

```{r}

# Prep the dataframe for drift function 
drift <- df %>% 
  filter(corr == "temp_anomalous")

#Define dates of interest for the FIRST portion of data
start<-mdy_hm("9-12-2020 12:00")
end<-mdy_hm("9-19-2020 13:30")
cleaned<-mdy_hm("9-19-2020 14:30")

#run function
drift1 <- fun_drift(drift, start, end, cleaned)

#Define dates of interest for the SECOND portion of data

start<-mdy_hm("9-19-2020 13:30")
end<-mdy_hm("10-9-2020 12:45")
cleaned<-mdy_hm("10-9-2020 16:00")


drift2 <- fun_drift(drift, start, end, cleaned)


#Replace values in master df
df<-bind_rows(df, drift1, drift2) %>% 
  filter(!corr == "temp")

(ggplot(data = df,
       mapping = aes(x = Timestamp, 
                     y = value,
                     color = corr)) +
  geom_line() +
  theme_bw())

```



## 6. Using the roll mean function to dampen noise. 

It may be worthwhile dropping, or flagging values that deviate too much from the rolling mean. The process for QAQC could be using the roll mean function, and plotting the residuals. Based on the difference between actual values and roll mean, we could set a threshold to dampen noise. 
```{r}

smooth <- df %>% 
  filter(corr == "temp_anomalous_drift") %>% 
  mutate(value = rollmean(value,
                          8,
                          fill = NA,
                          align = "center")) %>% 
  mutate(corr = "final_smooth")

df <- rbind(smooth, df) %>% 
  mutate("row" = row_number())

df_wide <- pivot_wider(df, 
                       names_from = corr, 
                       values_from = value) %>% 
  select(c(Timestamp, final_smooth, raw))

df_xts <- df_wide %>% 
  xts(df_wide, order.by = df_wide$Timestamp)

(dygraph(data = df_xts, main = "Example of fDOM corrections") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1, connectSeparatedPoints = TRUE) %>% 
  dyHighlight(highlightSeriesOpts = list(strokeWidth = 2)) %>% 
  dyRangeSelector())





```

```{r}


```